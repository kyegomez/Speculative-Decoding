{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Notebook\n",
    "\n",
    "## Experiment Environment\n",
    "- Single node with 8 NVIDIA RTX A6000 GPUs (48GB RAM)\n",
    "\n",
    "## Tasks: \n",
    "- Token generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model configurations: \n",
    "<!-- - $M_p: $ llama2 70b, $M_q: $ llamma-2 70b\n",
    "- $M_p: $ llama2 70b, $M_q: $ llamma-2 7b\n",
    "- $M_p: $ llama2 7b, $M_q: $ llamma-2 7b\n",
    "- $M_p: $ llama 30b, $M_q: $ llamma 30b\n",
    "- $M_p: $ llama 30b, $M_q: $ llamma 7b\n",
    "- $M_p: $ llama 1b, $M_q: $ llamma 1b -->\n",
    "\n",
    "- $M_p: $ Orca-2 7b, $M_q: $ Orca-2 7b\n",
    "- $M_p: $ Orca-2 7b, $M_q: $ Orca-2 13b\n",
    "- $M_p: $ Orca-2 13b, $M_q: $ Orca-2 13b\n",
    "\n",
    "- $M_p: $ bloom 7b, $M_q: $ bloom 7b\n",
    "- $M_p: $ bloom 7b, $M_q: $ bloom 560m\n",
    "- $M_p: $ bloom 560b, $M_q: $ bloom 560m\n",
    "\n",
    "- $M_p: $ baichuan 7b, $M_q: $ baichuan 7b\n",
    "- $M_p: $ baichuan 13b, $M_q: $ baichuan 7b\n",
    "- $M_p: $ baichuan 13b, $M_q: $ baichuan 13b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_configs = [\n",
    "#     (\"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models/Baichuan-7B\", \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models/Baichuan-7B\"), \n",
    "#     (\"baichuan-inc/Baichuan-7B\", \"baichuan-inc/Baichuan2-13B-Base\"), \n",
    "#     (\"baichuan-inc/Baichuan2-13B-Base\", \"baichuan-inc/Baichuan2-13B-Base\"), \n",
    "#     (\"microsoft/Orca-2-7b\", \"microsoft/Orca-2-7b\"),\n",
    "#     (\"microsoft/Orca-2-7b\", \"microsoft/Orca-2-13b\"),\n",
    "#     (\"microsoft/Orca-2-13b\", \"microsoft/Orca-2-13b\"),\n",
    "#     (\"bigscience/bloom-560m\", \"bigscience/bloom-560m\"),\n",
    "#     (\"bigscience/bloom-560m\", \"bigscience/bloom-7b1\"),\n",
    "#     (\"bigscience/bloom-7b1\", \"bigscience/bloom-7b1\"),\n",
    "# ]\n",
    "model_configs = [\n",
    "    (\"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Baichuan-7B\", \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Baichuan-7B\"), \n",
    "    (\"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Baichuan-7B\", \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Baichuan2-13B-Base\"), \n",
    "    (\"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Baichuan2-13B-Base\", \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Baichuan2-13B-Base\"), \n",
    "    (\"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Orca-2-7b\", \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Orca-2-7b\"),\n",
    "    (\"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Orca-2-7b\", \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Orca-2-13b\"),\n",
    "    (\"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Orca-2-13b\", \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Orca-2-13b\"),\n",
    "    (\"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloom-560m\", \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloom-560m\"),\n",
    "    (\"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloom-560m\", \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloom-7b1\"),\n",
    "    (\"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloom-7b1\", \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloom-7b1\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # \"llama1b\": \"/share_nfs/fangjiarui/root/code/hf_models/TinyLlama-1.1B-step-50K-105b\",\n",
    "    # \"llama7b\": \"/share_nfs/tianzhi/code/llama-7b\",\n",
    "    # \"llama30b\": \"/share_nfs/fangjiarui/root/code/hf_models/llama-30b-hf\",\n",
    "    # \"llama2-7b\" : \"/share_nfs/fangjiarui/root/code/hf_models/llama-2-7b-hf\",\n",
    "    # \"llama2-70b\" : \"/share_nfs/fangjiarui/root/code/hf_models/llama-2-70b-hf\",\n",
    "    # \"bloom-560m\": \"/share_nfs/fangjiarui/root/code/hf_models/bloom-560m\",\n",
    "    # \"bloom7b\": \"/share_nfs/fangjiarui/root/code/hf_models/bloomz-7b1\",\n",
    "    # \"baichuan-7b\": \"/share_nfs/duanqiyuan/models/source_models/hf/baichuan-7B\",\n",
    "    # \"baichuan-13b\": \"/share_nfs/duanqiyuan/models/source_models/hf/Baichuan-13B-Base\","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gamma vs Alpha over all model configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    \"What are the best vegan restaurants in New York City?\"\n",
    "    \"How can I improve my time management skills?\"\n",
    "    \"Explain the theory of relativity in simple terms.\",\n",
    "    \"Suggest some exercises for beginners in yoga.\",\n",
    "    \"What are the top five science fiction books of the last decade?\"]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 28 16:11:19 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  Off  | 00000000:03:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    73W / 300W |  74522MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80G...  Off  | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   60C    P0   264W / 300W |  31232MiB / 81920MiB |     98%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80G...  Off  | 00000000:43:00.0 Off |                    0 |\n",
      "| N/A   59C    P0   265W / 300W |  31232MiB / 81920MiB |     98%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80G...  Off  | 00000000:44:00.0 Off |                    0 |\n",
      "| N/A   61C    P0   275W / 300W |  31232MiB / 81920MiB |     98%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3872938      C   ..._env/llm_env39/bin/python    74520MiB |\n",
      "|    1   N/A  N/A   3912630      C   python                          31230MiB |\n",
      "|    2   N/A  N/A   3914305      C   python                          31230MiB |\n",
      "|    3   N/A  N/A   3914469      C   python                          31230MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import generate \n",
    "# from dist_inf import generate\n",
    "import pandas as pd\n",
    "\n",
    "gamma_vals = [i for i in range(4, 8)]\n",
    "gpu_num = 0\n",
    "df = pd.DataFrame(columns=['approx_model_name', 'target_model_name', 'gamma', 'alpha', 'approx_tokens_p_sec', 'target_tokens_p_second', 'sp_tokens_p_second', 'input_str_index'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1701206050.6472938"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin loading models: \n",
      " /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Baichuan-7B \n",
      " /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/Baichuan-7B\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model_configs)):\n",
    "    for txt_idx in range(len(input_texts)):\n",
    "        for j in gamma_vals:\n",
    "\n",
    "            try: \n",
    "                alpha, sp_tok_p_sec, approx_tok_p_sec, target_tok_p_sec = generate(\n",
    "                    input_text=input_texts[txt_idx], approx_model_name=model_configs[i][0],\n",
    "                    target_model_name=model_configs[i][1], random_seed=0, num_tokens=50,\n",
    "                    gamma=j,\n",
    "                    use_benchmark=True,\n",
    "                    gpu_num=gpu_num)\n",
    "            except RuntimeError as e:\n",
    "                # handle the error\n",
    "                print(e)\n",
    "\n",
    "                gpu_num+= 1\n",
    "                gpu_num %= 7\n",
    "\n",
    "                alpha, sp_tok_p_sec, approx_tok_p_sec, target_tok_p_sec = generate(\n",
    "                    input_text=input_texts[txt_idx], approx_model_name=model_configs[i][0],\n",
    "                    target_model_name=model_configs[i][1], random_seed=0, num_tokens=50,\n",
    "                    gamma=j,\n",
    "                    use_benchmark=True,\n",
    "                    gpu_num=gpu_num)\n",
    "\n",
    "            \n",
    "            new_row = pd.DataFrame({\n",
    "                'approx_model_name': [model_configs[i][0]], \n",
    "                'target_model_name': [model_configs[i][1]], \n",
    "                'gamma': [j],\n",
    "                'alpha': [alpha],\n",
    "                'approx_tokens_p_sec': [approx_tok_p_sec],\n",
    "                'target_tokens_p_second': [target_tok_p_sec],\n",
    "                'sp_tokens_p_second': [sp_tok_p_sec],\n",
    "                'input_str_index': [txt_idx]\n",
    "            }, index=[0])\n",
    "\n",
    "\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "            df.to_csv(f'exp_singleGPU.csv')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_vals = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dist inference\n",
    "def run_exp():\n",
    "    for i in range(len(model_configs)):\n",
    "        for txt_idx in range(len(input_texts)):\n",
    "            for j in gamma_vals:\n",
    "\n",
    "                try: \n",
    "                    res = generate(\n",
    "                        input_text=input_texts[txt_idx], approx_model_name=model_configs[i][0],\n",
    "                        target_model_name=model_configs[i][1], random_seed=0, num_tokens=50,\n",
    "                        gamma=j,\n",
    "                        use_benchmark=True)\n",
    "                except RuntimeError as e:\n",
    "                    # handle the error\n",
    "                    print(e)\n",
    "\n",
    "                    gpu_num+= 1\n",
    "                    gpu_num %= 7\n",
    "\n",
    "                    res = generate(\n",
    "                        input_text=input_texts[txt_idx], approx_model_name=model_configs[i][0],\n",
    "                        target_model_name=model_configs[i][1], random_seed=0, num_tokens=50,\n",
    "                        gamma=j,\n",
    "                        use_benchmark=True)\n",
    "\n",
    "                \n",
    "                print(res)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "begin loading models: \n",
      " /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloomz-7b1 \n",
      " /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloom-560m\n",
      "begin loading models: \n",
      " /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloomz-7b1 \n",
      " /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloom-560m\n",
      "begin loading models: \n",
      " /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloomz-7b1 \n",
      " /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloom-560m\n",
      "begin loading models: \n",
      " /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloomz-7b1 \n",
      " /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloom-560m\n",
      "Traceback (most recent call last):\n",
      "  File \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/dist_inf.py\", line 210, in <module>\n",
      "    generate(args.input, args.approx_model_name, args.target_model_name, num_tokens=args.max_tokens, gamma=args.gamma,\n",
      "  File \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/dist_inf.py\", line 128, in generate\n",
      "    small_model = AutoModelForCausalLM.from_pretrained(approx_model_name, \n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 516, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3091, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3471, in _load_pretrained_model\n",
      "    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 736, in _load_state_dict_into_meta_model\n",
      "    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/accelerate/utils/modeling.py\", line 317, in set_module_tensor_to_device\n",
      "    new_value = value.to(device)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacty of 47.54 GiB of which 38.00 MiB is free. Process 3257859 has 1.29 GiB memory in use. Process 3684696 has 20.79 GiB memory in use. Process 1077940 has 7.21 GiB memory in use. Process 1077943 has 7.12 GiB memory in use. Including non-PyTorch memory, this process has 3.99 GiB memory in use. Process 1077942 has 7.12 GiB memory in use. Of the allocated memory 3.25 GiB is allocated by PyTorch, and 1.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/dist_inf.py\", line 210, in <module>\n",
      "    generate(args.input, args.approx_model_name, args.target_model_name, num_tokens=args.max_tokens, gamma=args.gamma,\n",
      "  File \"/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/dist_inf.py\", line 134, in generate\n",
      "    large_model = AutoModelForCausalLM.from_pretrained(target_model_name, \n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 516, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3091, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3471, in _load_pretrained_model\n",
      "    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 736, in _load_state_dict_into_meta_model\n",
      "    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/accelerate/utils/modeling.py\", line 317, in set_module_tensor_to_device\n",
      "    new_value = value.to(device)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 47.54 GiB of which 18.00 MiB is free. Process 3257859 has 1.29 GiB memory in use. Process 3684696 has 20.79 GiB memory in use. Including non-PyTorch memory, this process has 7.21 GiB memory in use. Process 1077943 has 7.12 GiB memory in use. Process 1077941 has 3.99 GiB memory in use. Process 1077942 has 7.12 GiB memory in use. Of the allocated memory 6.47 GiB is allocated by PyTorch, and 4.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[2023-11-28 16:00:54,846] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1077941 closing signal SIGTERM\n",
      "[2023-11-28 16:00:54,847] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1077942 closing signal SIGTERM\n",
      "[2023-11-28 16:00:54,847] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1077943 closing signal SIGTERM\n",
      "[2023-11-28 16:00:55,576] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1077940) of binary: /u/ahs5ce/.conda/envs/spec_decode/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
      "    args.func(args)\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 985, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 654, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/torch/distributed/run.py\", line 797, in run\n",
      "    elastic_launch(\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/u/ahs5ce/.conda/envs/spec_decode/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "dist_inf.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-11-28_16:00:54\n",
      "  host      : groupml02\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1077940)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --num_processes=4 dist_inf.py \\\n",
    "    --target_model_name /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloom-560m \\\n",
    "    --approx_model_name /p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/models1/bloomz-7b1 -b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 28 16:07:51 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  Off  | 00000000:03:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    73W / 300W |  74522MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80G...  Off  | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   60C    P0   270W / 300W |  31232MiB / 81920MiB |     94%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80G...  Off  | 00000000:43:00.0 Off |                    0 |\n",
      "| N/A   59C    P0   269W / 300W |  31232MiB / 81920MiB |     97%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80G...  Off  | 00000000:44:00.0 Off |                    0 |\n",
      "| N/A   60C    P0   277W / 300W |  31232MiB / 81920MiB |     97%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3872938      C   ..._env/llm_env39/bin/python    74520MiB |\n",
      "|    1   N/A  N/A   3912630      C   python                          31230MiB |\n",
      "|    2   N/A  N/A   3914305      C   python                          31230MiB |\n",
      "|    3   N/A  N/A   3914469      C   python                          31230MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_exp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/experiments.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgroupml01.cs.virginia.edu/p/realai/amir/Speculative-Decoding/LLMSpeculativeSampling/experiments.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m run_exp()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_exp' is not defined"
     ]
    }
   ],
   "source": [
    "run_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('exp1_res.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gamma vs Tokens Generated per Second \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GPU Parallelization vs No GPU Parallelization Measuring Generatived per Second (gamma = 7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/u/ahs5ce/.cache/huggingface/hub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from transformers import file_utils\n",
    "print(file_utils.default_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec_decode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
